version: "3.8"

services:
  # AdvenceRAG Backend Service
  advence-rag:
    build:
      context: .
      dockerfile: Dockerfile
      target: search
    container_name: advence-rag
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      # Google API Keys
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - GOOGLE_SEARCH_API_KEY=${GOOGLE_SEARCH_API_KEY:-}
      - GOOGLE_SEARCH_CSE_ID=${GOOGLE_SEARCH_CSE_ID:-}
      # Web Search Provider
      - SEARCH_PROVIDER=${SEARCH_PROVIDER:-serper}
      - SERPER_API_KEY=${SERPER_API_KEY:-}
      # Vector Database
      - VECTOR_DB_TYPE=${VECTOR_DB_TYPE:-chroma}
      - CHROMA_PERSIST_DIRECTORY=/app/data/chroma
      - CHROMA_COLLECTION_NAME=${CHROMA_COLLECTION_NAME:-knowledge_base}
      - QDRANT_URL=${QDRANT_URL:-http://qdrant:6333}
      # LLM Settings
      - LLM_MODEL=${LLM_MODEL:-gemini-2.0-flash}
      - LLM_TEMPERATURE=${LLM_TEMPERATURE:-0.7}
      # Embedding
      - EMBEDDING_TYPE=${EMBEDDING_TYPE:-cloud}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-models/text-embedding-004}
      - LOCAL_EMBEDDING_DEVICE=cpu # 查詢服務固定用 CPU 或 API
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      # Persist vector database, sessions and shared ingestion queue
      - advence-rag-data:/app/data
      - advence-rag-sessions:/app/.sessions
      - advence-rag-ingest:/app/data/ingest
    networks:
      - rag-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # AdvenceRAG Ingestion Service (Heavyweight)
  # Background worker that processes files uploaded to /app/data/ingest
  advence-rag-ingest:
    build:
      context: .
      dockerfile: Dockerfile
      target: ingest
    container_name: advence-rag-ingest
    restart: unless-stopped
    environment:
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - VECTOR_DB_TYPE=${VECTOR_DB_TYPE:-chroma}
      - CHROMA_PERSIST_DIRECTORY=/app/data/chroma
      - QDRANT_URL=${QDRANT_URL:-http://qdrant:6333}
      - EMBEDDING_TYPE=${EMBEDDING_TYPE:-cloud}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-models/text-embedding-004}
      - LOCAL_EMBEDDING_DEVICE=cuda # 入庫服務強迫使用 CUDA
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - INGEST_INTERVAL=${INGEST_INTERVAL:-5}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: ["python", "-m", "advence_rag.cli", "scheduler", "--watch", "/app/data/ingest", "--interval", "${INGEST_INTERVAL:-5}"]
    volumes:
      - advence-rag-data:/app/data
      - advence-rag-ingest:/app/data/ingest
    networks:
      - rag-network

  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant-data:/qdrant/storage
    networks:
      - rag-network
    # Only start if specifically requested (or if you want it always on)
    # profiles: ["qdrant"] 

  # Open WebUI - ChatGPT-like interface
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui-docker-compose
    restart: unless-stopped
    ports:
      - "3000:8080"
    environment:
      # Disable built-in Ollama since we use external API
      - OLLAMA_BASE_URL=
      # Point to AdvenceRAG's OpenAI-compatible API
      - OPENAI_API_BASE_URL=http://advence-rag:8000/v1
      - OPENAI_API_KEY=${GOOGLE_API_KEY:-sk-placeholder}
      # WebUI settings
      - WEBUI_NAME=AdvenceRAG
      - ENABLE_SIGNUP=true
      - DEFAULT_USER_ROLE=user
    volumes:
      - open-webui-data:/app/backend/data
    networks:
      - rag-network
    depends_on:
      advence-rag:
        condition: service_healthy

volumes:
  advence-rag-data:
    driver: local
  advence-rag-sessions:
    driver: local
  advence-rag-ingest:
    driver: local
  open-webui-data:
    driver: local
  qdrant-data:
    driver: local

networks:
  rag-network:
    driver: bridge
